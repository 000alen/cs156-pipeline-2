{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alen/anaconda3/envs/cs156/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import mailbox\n",
    "import email\n",
    "import logging\n",
    "\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from typing import List, Optional, Any\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For topic modeling\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Additional imports from your pipeline\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"email_generation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7b665ff6f5b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint utilities (from your pipeline)\n",
    "CHECKPOINT_DIR = Path(\"checkpoints\")\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(obj: Any, name: str):\n",
    "    \"\"\"Saves an object to the checkpoint directory.\"\"\"\n",
    "    path = CHECKPOINT_DIR / f\"{name}.pkl\"\n",
    "    joblib.dump(obj, path)\n",
    "    logger.info(f\"Checkpoint saved: {path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(name: str) -> Optional[Any]:\n",
    "    \"\"\"Loads an object from the checkpoint directory.\"\"\"\n",
    "    path = CHECKPOINT_DIR / f\"{name}.pkl\"\n",
    "    if path.exists():\n",
    "        obj = joblib.load(path)\n",
    "        logger.info(f\"Checkpoint loaded: {path}\")\n",
    "        return obj\n",
    "    else:\n",
    "        logger.info(f\"No checkpoint found for: {path}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to parse and preprocess emails (from your pipeline)\n",
    "_GMAIL_CATEGORY_HEADER_MARKER = \"CategorÃ­a:\"\n",
    "\n",
    "\n",
    "def decode_mime_str(encoded: str) -> str:\n",
    "    # Decodes a MIME-encoded string to a regular string.\n",
    "    if not encoded:\n",
    "        return \"\"\n",
    "    fragments = email.header.decode_header(encoded)\n",
    "    decoded = \"\".join(\n",
    "        (\n",
    "            fragment.decode(charset or \"utf-8\", errors=\"ignore\")\n",
    "            if isinstance(fragment, bytes)\n",
    "            else fragment\n",
    "        )\n",
    "        for fragment, charset in fragments\n",
    "    )\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def parse_gmail_labels(gmail_labels: str) -> str:\n",
    "    # Parses Gmail labels to extract the primary category.\n",
    "    gmail_labels = gmail_labels.split(\",\")\n",
    "    category_label = \"Uncategorized\"\n",
    "    for label in gmail_labels:\n",
    "        if _GMAIL_CATEGORY_HEADER_MARKER in label:\n",
    "            category_label = label.replace(_GMAIL_CATEGORY_HEADER_MARKER, \"\").strip()\n",
    "            break\n",
    "    return category_label\n",
    "\n",
    "\n",
    "def parse_body(message: mailbox.Message) -> str:\n",
    "    # Extracts and decodes the body of an email message.\n",
    "    body_parts = []\n",
    "    try:\n",
    "        if message.is_multipart():\n",
    "            for part in message.walk():\n",
    "                payload = part.get_payload(decode=True)\n",
    "                if payload:\n",
    "                    body_parts.append(payload.decode(\"utf-8\", errors=\"ignore\"))\n",
    "        else:\n",
    "            payload = message.get_payload(decode=True)\n",
    "            if payload:\n",
    "                body_parts.append(payload.decode(\"utf-8\", errors=\"ignore\"))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting body: {e}\")\n",
    "    return \" \".join(body_parts)\n",
    "\n",
    "\n",
    "def parse_message(message) -> tuple[str, str, str]:\n",
    "    # Parses an email message to extract the subject, body, and category.\n",
    "    try:\n",
    "        subject = decode_mime_str(message.get(\"subject\", \"\"))\n",
    "        body = parse_body(message)\n",
    "        category = parse_gmail_labels(\n",
    "            decode_mime_str(message.get(\"X-Gmail-Labels\", \"\"))\n",
    "        )\n",
    "        return [subject, body, category]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process an email: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_emails(mbox_file_path: str, max_emails: int | None = None) -> pd.DataFrame:\n",
    "    # Loads and parses emails from an MBOX file using parallel processing.\n",
    "    columns = [\"Subject\", \"Body\", \"Category\"]\n",
    "    data = []\n",
    "    mbox = mailbox.mbox(mbox_file_path)\n",
    "    n_processes = max(cpu_count() - 1, 1)\n",
    "    with Pool(processes=n_processes) as pool:\n",
    "        for i, result in enumerate(tqdm(pool.imap_unordered(parse_message, mbox, chunksize=100))):\n",
    "            if result:\n",
    "                data.append(result)\n",
    "            if max_emails is not None and len(data) >= max_emails:\n",
    "                break\n",
    "    return pd.DataFrame(data, columns=columns)\n",
    "\n",
    "\n",
    "# Preprocessing functions (from your pipeline)\n",
    "def parse_html(html: str) -> str:\n",
    "    # Parses HTML content and extracts text.\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "\n",
    "def transform_parse_html(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    columns: List[str] | None = None,\n",
    "):\n",
    "    if columns is None:\n",
    "        columns = [\"Body\"]\n",
    "    for column in columns:\n",
    "        df[column] = df[column].apply(lambda x: parse_html(x))\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # Normalizes the text by converting to lowercase.\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "def transform_clean_text(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    columns: List[str] | None = None,\n",
    "):\n",
    "    if columns is None:\n",
    "        columns = [\"Subject\", \"Body\"]\n",
    "    for column in columns:\n",
    "        df[column] = df[column].apply(lambda x: clean_text(x))\n",
    "\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Preprocesses the email data by cleaning text.\n",
    "    transform_parse_html(df)\n",
    "    transform_clean_text(df)\n",
    "    df[\"Text\"] = df[\"Subject\"] + \" \" + df[\"Body\"]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:email_generation:Checkpoint loaded: checkpoints/emails_df.pkl\n",
      "INFO:email_generation:Emails DataFrame loaded from checkpoint.\n"
     ]
    }
   ],
   "source": [
    "# Load the emails\n",
    "mbox_path = Path(\"emails.mbox\")\n",
    "df = load_checkpoint(\"emails_df\")\n",
    "if df is not None:\n",
    "    logger.info(\"Emails DataFrame loaded from checkpoint.\")\n",
    "else:\n",
    "    df = load_emails(mbox_path, max_emails=100)\n",
    "    save_checkpoint(df, \"emails_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:email_generation:No checkpoint found for: checkpoints/preprocessed_df.pkl\n",
      "INFO:email_generation:Checkpoint saved: checkpoints/preprocessed_df.pkl\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing\n",
    "preprocessed_df = load_checkpoint(\"preprocessed_df\")\n",
    "if preprocessed_df is not None:\n",
    "    df = preprocessed_df\n",
    "    logger.info(\"Preprocessed DataFrame loaded from checkpoint.\")\n",
    "else:\n",
    "    df = preprocess_data(df)\n",
    "    save_checkpoint(df, \"preprocessed_df\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Topic Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess texts for topic modeling\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def preprocess_text_for_topic_modeling(text):\n",
    "    tokens = simple_preprocess(text, deacc=True)  # deacc=True removes punctuations\n",
    "    tokens = [t for t in tokens if t not in STOP_WORDS]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Convert topic distributions to fixed-size vectors\n",
    "def topic_vector(topic_dist, num_topics):\n",
    "    vec = np.zeros(num_topics)\n",
    "    for topic_id, prob in topic_dist:\n",
    "        vec[topic_id] = prob\n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:email_generation:No checkpoint found for: checkpoints/processed_texts.pkl\n",
      "Processing texts: 100%|ââââââââââ| 100/100 [00:00<00:00, 731.33it/s]\n",
      "INFO:email_generation:Checkpoint saved: checkpoints/processed_texts.pkl\n"
     ]
    }
   ],
   "source": [
    "# Extract the texts for topic modeling\n",
    "texts = df[\"Text\"].tolist()\n",
    "\n",
    "processed_texts = load_checkpoint(\"processed_texts\")\n",
    "if processed_texts is None:\n",
    "    processed_texts = [preprocess_text_for_topic_modeling(text) for text in tqdm(texts, desc=\"Processing texts\")]\n",
    "    save_checkpoint(processed_texts, \"processed_texts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:email_generation:No checkpoint found for: checkpoints/dictionary.pkl\n",
      "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary<0 unique tokens: []>\n",
      "INFO:gensim.corpora.dictionary:built Dictionary<14975 unique tokens: ['ae', 'afterhours', 'aires', 'argentino', 'auto']...> from 100 documents (total 63329 corpus positions)\n",
      "INFO:gensim.utils:Dictionary lifecycle event {'msg': \"built Dictionary<14975 unique tokens: ['ae', 'afterhours', 'aires', 'argentino', 'auto']...> from 100 documents (total 63329 corpus positions)\", 'datetime': '2024-12-03T12:29:33.466670', 'gensim': '4.3.3', 'python': '3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'created'}\n",
      "INFO:email_generation:Checkpoint saved: checkpoints/dictionary.pkl\n",
      "INFO:email_generation:No checkpoint found for: checkpoints/corpus.pkl\n",
      "Creating corpus: 100%|ââââââââââ| 100/100 [00:00<00:00, 6747.16it/s]\n",
      "INFO:email_generation:Checkpoint saved: checkpoints/corpus.pkl\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary and corpus for LDA\n",
    "\n",
    "dictionary = load_checkpoint(\"dictionary\")\n",
    "if dictionary is None:\n",
    "    dictionary = corpora.Dictionary(processed_texts)\n",
    "    save_checkpoint(dictionary, \"dictionary\")\n",
    "\n",
    "corpus = load_checkpoint(\"corpus\")\n",
    "if corpus is None:\n",
    "    corpus = [dictionary.doc2bow(text) for text in tqdm(processed_texts, desc=\"Creating corpus\")]\n",
    "    save_checkpoint(corpus, \"corpus\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:email_generation:No checkpoint found for: checkpoints/lda_model.pkl\n",
      "INFO:gensim.models.ldamodel:using symmetric alpha at 0.2\n",
      "INFO:gensim.models.ldamodel:using symmetric eta at 0.2\n",
      "INFO:gensim.models.ldamodel:using serial LDA version on this node\n",
      "INFO:gensim.models.ldamodel:running online (multi-pass) LDA training, 5 topics, 15 passes over the supplied corpus of 100 documents, updating model once every 100 documents, evaluating perplexity every 100 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO:gensim.models.ldamodel:-10.900 per-word bound, 1910.2 perplexity estimate based on a held-out corpus of 100 documents with 63329 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #100/100\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.200): 0.013*\"https\" + 0.011*\"com\" + 0.007*\"click\" + 0.006*\"email\" + 0.005*\"utm_source\" + 0.005*\"read\" + 0.003*\"jamanetwork\" + 0.003*\"ks\" + 0.003*\"alerts\" + 0.003*\"app\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.200): 0.009*\"https\" + 0.006*\"com\" + 0.005*\"de\" + 0.004*\"utm_source\" + 0.004*\"read\" + 0.004*\"en\" + 0.004*\"email\" + 0.004*\"tldr\" + 0.003*\"data\" + 0.003*\"minute\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.200): 0.009*\"com\" + 0.009*\"https\" + 0.005*\"de\" + 0.004*\"email\" + 0.003*\"read\" + 0.003*\"click\" + 0.003*\"new\" + 0.003*\"github\" + 0.003*\"us\" + 0.003*\"en\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.200): 0.012*\"https\" + 0.011*\"com\" + 0.007*\"github\" + 0.007*\"data\" + 0.006*\"build\" + 0.006*\"de\" + 0.005*\"root\" + 0.004*\"eigen\" + 0.004*\"run\" + 0.004*\"failed\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.200): 0.017*\"de\" + 0.014*\"https\" + 0.014*\"com\" + 0.007*\"la\" + 0.006*\"email\" + 0.005*\"en\" + 0.005*\"utm_source\" + 0.004*\"read\" + 0.004*\"el\" + 0.004*\"utm_medium\"\n",
      "INFO:gensim.models.ldamodel:topic diff=1.787687, rho=1.000000\n",
      "INFO:gensim.models.ldamodel:-9.051 per-word bound, 530.3 perplexity estimate based on a held-out corpus of 100 documents with 63329 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 1, at document #100/100\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.200): 0.016*\"https\" + 0.016*\"com\" + 0.012*\"click\" + 0.007*\"jamanetwork\" + 0.006*\"alerts\" + 0.006*\"email\" + 0.006*\"ks\" + 0.006*\"axac\" + 0.006*\"objz\" + 0.005*\"read\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.200): 0.008*\"https\" + 0.007*\"read\" + 0.005*\"tldr\" + 0.005*\"com\" + 0.005*\"nvidia\" + 0.004*\"utm_source\" + 0.004*\"de\" + 0.004*\"data\" + 0.004*\"minute\" + 0.004*\"new\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.200): 0.005*\"com\" + 0.005*\"https\" + 0.003*\"email\" + 0.003*\"obj\" + 0.003*\"de\" + 0.003*\"endobj\" + 0.002*\"type\" + 0.002*\"ai\" + 0.002*\"us\" + 0.002*\"null\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.200): 0.013*\"https\" + 0.012*\"com\" + 0.011*\"github\" + 0.010*\"build\" + 0.009*\"data\" + 0.007*\"root\" + 0.007*\"eigen\" + 0.006*\"failed\" + 0.006*\"run\" + 0.004*\"workflow\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.200): 0.019*\"de\" + 0.015*\"https\" + 0.013*\"com\" + 0.007*\"la\" + 0.007*\"utm_source\" + 0.007*\"email\" + 0.006*\"en\" + 0.005*\"el\" + 0.005*\"read\" + 0.005*\"utm_medium\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.901942, rho=0.577350\n",
      "INFO:gensim.models.ldamodel:-8.560 per-word bound, 377.4 perplexity estimate based on a held-out corpus of 100 documents with 63329 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 2, at document #100/100\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.200): 0.018*\"com\" + 0.018*\"https\" + 0.015*\"click\" + 0.008*\"jamanetwork\" + 0.007*\"alerts\" + 0.007*\"ks\" + 0.007*\"axac\" + 0.007*\"objz\" + 0.006*\"email\" + 0.005*\"heygen\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.200): 0.009*\"read\" + 0.008*\"https\" + 0.006*\"nvidia\" + 0.006*\"tldr\" + 0.005*\"com\" + 0.005*\"minute\" + 0.004*\"utm_source\" + 0.004*\"data\" + 0.004*\"new\" + 0.004*\"de\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.200): 0.004*\"com\" + 0.004*\"https\" + 0.003*\"obj\" + 0.003*\"endobj\" + 0.003*\"type\" + 0.002*\"null\" + 0.002*\"email\" + 0.002*\"ai\" + 0.002*\"length\" + 0.002*\"stream\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.200): 0.013*\"https\" + 0.013*\"github\" + 0.012*\"com\" + 0.012*\"build\" + 0.011*\"data\" + 0.008*\"eigen\" + 0.008*\"root\" + 0.007*\"failed\" + 0.006*\"run\" + 0.005*\"sans\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.200): 0.019*\"de\" + 0.015*\"https\" + 0.013*\"com\" + 0.007*\"utm_source\" + 0.007*\"la\" + 0.007*\"email\" + 0.007*\"en\" + 0.005*\"el\" + 0.005*\"read\" + 0.005*\"utm_medium\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.535387, rho=0.500000\n",
      "INFO:gensim.models.ldamodel:-8.405 per-word bound, 338.9 perplexity estimate based on a held-out corpus of 100 documents with 63329 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 3, at document #100/100\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.200): 0.019*\"com\" + 0.018*\"https\" + 0.016*\"click\" + 0.009*\"jamanetwork\" + 0.008*\"alerts\" + 0.008*\"ks\" + 0.007*\"axac\" + 0.007*\"objz\" + 0.006*\"email\" + 0.006*\"heygen\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.200): 0.010*\"read\" + 0.008*\"https\" + 0.007*\"nvidia\" + 0.006*\"tldr\" + 0.005*\"minute\" + 0.005*\"com\" + 0.005*\"data\" + 0.004*\"utm_source\" + 0.004*\"new\" + 0.004*\"ai\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.200): 0.004*\"obj\" + 0.004*\"endobj\" + 0.003*\"type\" + 0.003*\"com\" + 0.003*\"https\" + 0.002*\"null\" + 0.002*\"length\" + 0.002*\"stream\" + 0.002*\"ai\" + 0.002*\"copilot\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.200): 0.014*\"github\" + 0.014*\"https\" + 0.013*\"com\" + 0.012*\"build\" + 0.011*\"data\" + 0.009*\"eigen\" + 0.009*\"root\" + 0.008*\"failed\" + 0.007*\"run\" + 0.006*\"sans\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.200): 0.019*\"de\" + 0.015*\"https\" + 0.013*\"com\" + 0.008*\"utm_source\" + 0.007*\"email\" + 0.007*\"en\" + 0.007*\"la\" + 0.005*\"el\" + 0.005*\"read\" + 0.005*\"utm_medium\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.339894, rho=0.447214\n",
      "INFO:gensim.models.ldamodel:-8.342 per-word bound, 324.5 perplexity estimate based on a held-out corpus of 100 documents with 63329 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 4, at document #100/100\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.200): 0.019*\"com\" + 0.019*\"https\" + 0.016*\"click\" + 0.009*\"jamanetwork\" + 0.008*\"alerts\" + 0.008*\"ks\" + 0.008*\"axac\" + 0.008*\"objz\" + 0.006*\"heygen\" + 0.006*\"email\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.200): 0.010*\"read\" + 0.008*\"https\" + 0.007*\"nvidia\" + 0.006*\"tldr\" + 0.005*\"minute\" + 0.005*\"com\" + 0.005*\"data\" + 0.004*\"new\" + 0.004*\"utm_source\" + 0.004*\"ai\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.200): 0.004*\"obj\" + 0.004*\"endobj\" + 0.003*\"type\" + 0.003*\"null\" + 0.002*\"length\" + 0.002*\"com\" + 0.002*\"https\" + 0.002*\"stream\" + 0.002*\"copilot\" + 0.002*\"ai\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.200): 0.014*\"github\" + 0.014*\"https\" + 0.013*\"build\" + 0.013*\"com\" + 0.012*\"data\" + 0.010*\"eigen\" + 0.009*\"root\" + 0.008*\"failed\" + 0.007*\"run\" + 0.006*\"sans\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.200): 0.019*\"de\" + 0.015*\"https\" + 0.013*\"com\" + 0.008*\"utm_source\" + 0.007*\"email\" + 0.007*\"en\" + 0.007*\"la\" + 0.005*\"el\" + 0.005*\"read\" + 0.005*\"utm_medium\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.221655, rho=0.408248\n",
      "INFO:gensim.models.ldamodel:-8.313 per-word bound, 318.0 perplexity estimate based on a held-out corpus of 100 documents with 63329 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 5, at document #100/100\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.200): 0.020*\"com\" + 0.019*\"https\" + 0.016*\"click\" + 0.009*\"jamanetwork\" + 0.009*\"alerts\" + 0.008*\"ks\" + 0.008*\"axac\" + 0.008*\"objz\" + 0.006*\"heygen\" + 0.006*\"email\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.200): 0.010*\"read\" + 0.007*\"https\" + 0.007*\"nvidia\" + 0.006*\"tldr\" + 0.005*\"minute\" + 0.005*\"com\" + 0.005*\"data\" + 0.004*\"new\" + 0.004*\"utm_source\" + 0.004*\"ai\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.200): 0.004*\"obj\" + 0.004*\"endobj\" + 0.003*\"type\" + 0.003*\"null\" + 0.002*\"length\" + 0.002*\"stream\" + 0.002*\"copilot\" + 0.002*\"com\" + 0.002*\"ai\" + 0.002*\"filter\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.200): 0.015*\"github\" + 0.014*\"https\" + 0.013*\"build\" + 0.013*\"com\" + 0.012*\"data\" + 0.010*\"eigen\" + 0.009*\"root\" + 0.009*\"failed\" + 0.007*\"run\" + 0.006*\"sans\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.200): 0.019*\"de\" + 0.015*\"https\" + 0.013*\"com\" + 0.008*\"utm_source\" + 0.007*\"email\" + 0.007*\"en\" + 0.007*\"la\" + 0.005*\"el\" + 0.005*\"read\" + 0.005*\"tldr\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.147798, rho=0.377964\n",
      "INFO:gensim.models.ldamodel:-8.298 per-word bound, 314.7 perplexity estimate based on a held-out corpus of 100 documents with 63329 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 6, at document #100/100\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.200): 0.020*\"com\" + 0.019*\"https\" + 0.016*\"click\" + 0.009*\"jamanetwork\" + 0.009*\"alerts\" + 0.008*\"ks\" + 0.008*\"axac\" + 0.008*\"objz\" + 0.007*\"heygen\" + 0.006*\"aws\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.200): 0.011*\"read\" + 0.008*\"nvidia\" + 0.007*\"https\" + 0.006*\"tldr\" + 0.005*\"minute\" + 0.005*\"com\" + 0.005*\"data\" + 0.004*\"new\" + 0.004*\"utm_source\" + 0.004*\"ai\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.200): 0.004*\"obj\" + 0.004*\"endobj\" + 0.003*\"type\" + 0.003*\"null\" + 0.002*\"length\" + 0.002*\"stream\" + 0.002*\"copilot\" + 0.002*\"ai\" + 0.002*\"filter\" + 0.002*\"flatedecode\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.200): 0.015*\"github\" + 0.014*\"https\" + 0.013*\"build\" + 0.013*\"com\" + 0.012*\"data\" + 0.010*\"eigen\" + 0.009*\"root\" + 0.009*\"failed\" + 0.007*\"run\" + 0.006*\"sans\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.200): 0.019*\"de\" + 0.015*\"https\" + 0.013*\"com\" + 0.008*\"utm_source\" + 0.007*\"email\" + 0.007*\"en\" + 0.007*\"la\" + 0.005*\"el\" + 0.005*\"read\" + 0.005*\"tldr\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.100381, rho=0.353553\n",
      "INFO:gensim.models.ldamodel:-8.290 per-word bound, 312.9 perplexity estimate based on a held-out corpus of 100 documents with 63329 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 7, at document #100/100\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.200): 0.020*\"com\" + 0.019*\"https\" + 0.017*\"click\" + 0.009*\"jamanetwork\" + 0.009*\"alerts\" + 0.008*\"ks\" + 0.008*\"axac\" + 0.008*\"objz\" + 0.007*\"heygen\" + 0.006*\"aws\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.200): 0.011*\"read\" + 0.008*\"nvidia\" + 0.007*\"https\" + 0.006*\"tldr\" + 0.005*\"minute\" + 0.005*\"com\" + 0.005*\"data\" + 0.004*\"new\" + 0.004*\"utm_source\" + 0.004*\"ai\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.200): 0.004*\"obj\" + 0.004*\"endobj\" + 0.003*\"type\" + 0.003*\"null\" + 0.003*\"length\" + 0.002*\"stream\" + 0.002*\"copilot\" + 0.002*\"filter\" + 0.002*\"ai\" + 0.002*\"flatedecode\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.200): 0.015*\"github\" + 0.014*\"https\" + 0.013*\"build\" + 0.013*\"com\" + 0.012*\"data\" + 0.010*\"eigen\" + 0.009*\"root\" + 0.009*\"failed\" + 0.007*\"run\" + 0.006*\"sans\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.200): 0.019*\"de\" + 0.015*\"https\" + 0.012*\"com\" + 0.008*\"utm_source\" + 0.007*\"email\" + 0.007*\"en\" + 0.007*\"la\" + 0.005*\"el\" + 0.005*\"read\" + 0.005*\"tldr\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.069405, rho=0.333333\n",
      "INFO:gensim.models.ldamodel:-8.285 per-word bound, 311.8 perplexity estimate based on a held-out corpus of 100 documents with 63329 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 8, at document #100/100\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.200): 0.020*\"com\" + 0.019*\"https\" + 0.017*\"click\" + 0.009*\"jamanetwork\" + 0.009*\"alerts\" + 0.008*\"ks\" + 0.008*\"axac\" + 0.008*\"objz\" + 0.007*\"heygen\" + 0.006*\"aws\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.200): 0.011*\"read\" + 0.008*\"nvidia\" + 0.007*\"https\" + 0.006*\"tldr\" + 0.006*\"minute\" + 0.005*\"com\" + 0.005*\"data\" + 0.004*\"new\" + 0.004*\"utm_source\" + 0.004*\"ai\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.200): 0.004*\"obj\" + 0.004*\"endobj\" + 0.003*\"type\" + 0.003*\"null\" + 0.003*\"length\" + 0.002*\"stream\" + 0.002*\"copilot\" + 0.002*\"filter\" + 0.002*\"ai\" + 0.002*\"flatedecode\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.200): 0.015*\"github\" + 0.014*\"https\" + 0.013*\"build\" + 0.013*\"com\" + 0.012*\"data\" + 0.011*\"eigen\" + 0.009*\"root\" + 0.009*\"failed\" + 0.007*\"run\" + 0.006*\"sans\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.200): 0.019*\"de\" + 0.015*\"https\" + 0.012*\"com\" + 0.008*\"utm_source\" + 0.007*\"email\" + 0.007*\"en\" + 0.007*\"la\" + 0.005*\"el\" + 0.005*\"read\" + 0.005*\"tldr\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.048811, rho=0.316228\n",
      "INFO:gensim.models.ldamodel:-8.281 per-word bound, 311.1 perplexity estimate based on a held-out corpus of 100 documents with 63329 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 9, at document #100/100\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.200): 0.020*\"com\" + 0.019*\"https\" + 0.017*\"click\" + 0.009*\"jamanetwork\" + 0.009*\"alerts\" + 0.008*\"ks\" + 0.008*\"axac\" + 0.008*\"objz\" + 0.007*\"heygen\" + 0.006*\"aws\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.200): 0.011*\"read\" + 0.008*\"nvidia\" + 0.007*\"https\" + 0.006*\"tldr\" + 0.006*\"minute\" + 0.005*\"com\" + 0.005*\"data\" + 0.005*\"new\" + 0.004*\"utm_source\" + 0.004*\"ai\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.200): 0.004*\"obj\" + 0.004*\"endobj\" + 0.003*\"type\" + 0.003*\"null\" + 0.003*\"length\" + 0.002*\"stream\" + 0.002*\"copilot\" + 0.002*\"filter\" + 0.002*\"flatedecode\" + 0.002*\"ai\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.200): 0.015*\"github\" + 0.014*\"https\" + 0.013*\"build\" + 0.013*\"com\" + 0.012*\"data\" + 0.011*\"eigen\" + 0.009*\"root\" + 0.009*\"failed\" + 0.007*\"run\" + 0.006*\"sans\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.200): 0.020*\"de\" + 0.015*\"https\" + 0.012*\"com\" + 0.008*\"utm_source\" + 0.007*\"email\" + 0.007*\"en\" + 0.007*\"la\" + 0.005*\"el\" + 0.005*\"read\" + 0.005*\"tldr\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.035026, rho=0.301511\n",
      "INFO:gensim.models.ldamodel:-8.279 per-word bound, 310.6 perplexity estimate based on a held-out corpus of 100 documents with 63329 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 10, at document #100/100\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.200): 0.020*\"com\" + 0.020*\"https\" + 0.017*\"click\" + 0.009*\"jamanetwork\" + 0.009*\"alerts\" + 0.008*\"ks\" + 0.008*\"axac\" + 0.008*\"objz\" + 0.007*\"heygen\" + 0.006*\"aws\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.200): 0.011*\"read\" + 0.008*\"nvidia\" + 0.007*\"https\" + 0.006*\"tldr\" + 0.006*\"minute\" + 0.005*\"com\" + 0.005*\"data\" + 0.005*\"new\" + 0.004*\"utm_source\" + 0.004*\"ai\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.200): 0.004*\"obj\" + 0.004*\"endobj\" + 0.003*\"type\" + 0.003*\"null\" + 0.003*\"length\" + 0.002*\"stream\" + 0.002*\"copilot\" + 0.002*\"filter\" + 0.002*\"flatedecode\" + 0.002*\"ai\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.200): 0.015*\"github\" + 0.014*\"https\" + 0.013*\"build\" + 0.013*\"com\" + 0.012*\"data\" + 0.011*\"eigen\" + 0.009*\"root\" + 0.009*\"failed\" + 0.007*\"run\" + 0.006*\"sans\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.200): 0.020*\"de\" + 0.015*\"https\" + 0.012*\"com\" + 0.008*\"utm_source\" + 0.007*\"email\" + 0.007*\"en\" + 0.007*\"la\" + 0.005*\"el\" + 0.005*\"read\" + 0.005*\"tldr\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.025668, rho=0.288675\n",
      "INFO:gensim.models.ldamodel:-8.277 per-word bound, 310.1 perplexity estimate based on a held-out corpus of 100 documents with 63329 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 11, at document #100/100\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.200): 0.020*\"com\" + 0.020*\"https\" + 0.017*\"click\" + 0.009*\"jamanetwork\" + 0.009*\"alerts\" + 0.008*\"ks\" + 0.008*\"axac\" + 0.008*\"objz\" + 0.007*\"heygen\" + 0.006*\"aws\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.200): 0.011*\"read\" + 0.008*\"nvidia\" + 0.007*\"https\" + 0.006*\"tldr\" + 0.006*\"minute\" + 0.005*\"com\" + 0.005*\"data\" + 0.005*\"new\" + 0.004*\"utm_source\" + 0.004*\"ai\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.200): 0.004*\"obj\" + 0.004*\"endobj\" + 0.003*\"type\" + 0.003*\"null\" + 0.003*\"length\" + 0.002*\"stream\" + 0.002*\"copilot\" + 0.002*\"filter\" + 0.002*\"flatedecode\" + 0.002*\"xz\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.200): 0.015*\"github\" + 0.014*\"https\" + 0.013*\"build\" + 0.013*\"com\" + 0.012*\"data\" + 0.011*\"eigen\" + 0.009*\"root\" + 0.009*\"failed\" + 0.007*\"run\" + 0.006*\"holafly\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.200): 0.020*\"de\" + 0.015*\"https\" + 0.012*\"com\" + 0.008*\"utm_source\" + 0.008*\"email\" + 0.007*\"en\" + 0.007*\"la\" + 0.005*\"el\" + 0.005*\"read\" + 0.005*\"tldr\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.019111, rho=0.277350\n",
      "INFO:gensim.models.ldamodel:-8.275 per-word bound, 309.8 perplexity estimate based on a held-out corpus of 100 documents with 63329 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 12, at document #100/100\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.200): 0.021*\"com\" + 0.020*\"https\" + 0.016*\"click\" + 0.009*\"jamanetwork\" + 0.009*\"alerts\" + 0.008*\"ks\" + 0.008*\"axac\" + 0.008*\"objz\" + 0.007*\"heygen\" + 0.006*\"aws\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.200): 0.011*\"read\" + 0.008*\"nvidia\" + 0.007*\"https\" + 0.006*\"tldr\" + 0.006*\"minute\" + 0.005*\"com\" + 0.005*\"data\" + 0.005*\"new\" + 0.004*\"utm_source\" + 0.004*\"ai\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.200): 0.004*\"obj\" + 0.004*\"endobj\" + 0.003*\"type\" + 0.003*\"null\" + 0.003*\"length\" + 0.002*\"stream\" + 0.002*\"copilot\" + 0.002*\"filter\" + 0.002*\"flatedecode\" + 0.002*\"xz\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.200): 0.015*\"github\" + 0.014*\"https\" + 0.013*\"build\" + 0.013*\"com\" + 0.012*\"data\" + 0.011*\"eigen\" + 0.009*\"root\" + 0.009*\"failed\" + 0.007*\"run\" + 0.007*\"holafly\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.200): 0.020*\"de\" + 0.014*\"https\" + 0.012*\"com\" + 0.008*\"email\" + 0.008*\"utm_source\" + 0.007*\"en\" + 0.007*\"la\" + 0.005*\"el\" + 0.005*\"read\" + 0.005*\"tldr\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.014469, rho=0.267261\n",
      "INFO:gensim.models.ldamodel:-8.274 per-word bound, 309.5 perplexity estimate based on a held-out corpus of 100 documents with 63329 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 13, at document #100/100\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.200): 0.021*\"com\" + 0.020*\"https\" + 0.016*\"click\" + 0.009*\"jamanetwork\" + 0.009*\"alerts\" + 0.008*\"ks\" + 0.008*\"axac\" + 0.008*\"objz\" + 0.007*\"heygen\" + 0.006*\"aws\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.200): 0.011*\"read\" + 0.008*\"nvidia\" + 0.007*\"https\" + 0.006*\"tldr\" + 0.006*\"minute\" + 0.005*\"com\" + 0.005*\"data\" + 0.005*\"new\" + 0.004*\"utm_source\" + 0.004*\"ai\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.200): 0.004*\"obj\" + 0.004*\"endobj\" + 0.003*\"type\" + 0.003*\"null\" + 0.003*\"length\" + 0.002*\"stream\" + 0.002*\"copilot\" + 0.002*\"filter\" + 0.002*\"flatedecode\" + 0.002*\"xz\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.200): 0.015*\"github\" + 0.014*\"https\" + 0.013*\"build\" + 0.013*\"com\" + 0.012*\"data\" + 0.011*\"eigen\" + 0.009*\"root\" + 0.009*\"failed\" + 0.007*\"run\" + 0.007*\"holafly\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.200): 0.020*\"de\" + 0.014*\"https\" + 0.012*\"com\" + 0.008*\"email\" + 0.007*\"utm_source\" + 0.007*\"en\" + 0.007*\"la\" + 0.005*\"el\" + 0.005*\"read\" + 0.005*\"tldr\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.011110, rho=0.258199\n",
      "INFO:gensim.models.ldamodel:-8.273 per-word bound, 309.3 perplexity estimate based on a held-out corpus of 100 documents with 63329 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 14, at document #100/100\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.200): 0.021*\"com\" + 0.020*\"https\" + 0.016*\"click\" + 0.009*\"jamanetwork\" + 0.009*\"alerts\" + 0.008*\"ks\" + 0.008*\"axac\" + 0.008*\"objz\" + 0.007*\"heygen\" + 0.006*\"aws\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.200): 0.011*\"read\" + 0.008*\"nvidia\" + 0.007*\"https\" + 0.006*\"tldr\" + 0.006*\"minute\" + 0.005*\"com\" + 0.005*\"data\" + 0.005*\"new\" + 0.004*\"utm_source\" + 0.004*\"ai\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.200): 0.004*\"obj\" + 0.004*\"endobj\" + 0.003*\"type\" + 0.003*\"null\" + 0.003*\"length\" + 0.002*\"stream\" + 0.002*\"copilot\" + 0.002*\"filter\" + 0.002*\"flatedecode\" + 0.002*\"xz\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.200): 0.015*\"github\" + 0.014*\"https\" + 0.013*\"build\" + 0.013*\"com\" + 0.012*\"data\" + 0.011*\"eigen\" + 0.009*\"root\" + 0.009*\"failed\" + 0.007*\"run\" + 0.007*\"holafly\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.200): 0.020*\"de\" + 0.014*\"https\" + 0.012*\"com\" + 0.008*\"email\" + 0.007*\"utm_source\" + 0.007*\"en\" + 0.007*\"la\" + 0.005*\"el\" + 0.005*\"read\" + 0.005*\"tldr\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.008616, rho=0.250000\n",
      "INFO:gensim.utils:LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=14975, num_topics=5, decay=0.5, chunksize=2000> in 1.74s', 'datetime': '2024-12-03T12:29:35.393193', 'gensim': '4.3.3', 'python': '3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'created'}\n",
      "INFO:email_generation:Checkpoint saved: checkpoints/lda_model.pkl\n",
      "INFO:email_generation:No checkpoint found for: checkpoints/topic_distributions.pkl\n",
      "Getting topic distributions: 100%|ââââââââââ| 100/100 [00:00<00:00, 4486.46it/s]\n",
      "INFO:email_generation:Checkpoint saved: checkpoints/topic_distributions.pkl\n",
      "INFO:email_generation:No checkpoint found for: checkpoints/topic_vectors.pkl\n",
      "Creating topic vectors: 100%|ââââââââââ| 100/100 [00:00<00:00, 565270.08it/s]\n",
      "INFO:email_generation:Checkpoint saved: checkpoints/topic_vectors.pkl\n"
     ]
    }
   ],
   "source": [
    "# Train LDA model\n",
    "num_topics = 5  # Adjust based on your data\n",
    "lda_model = load_checkpoint(\"lda_model\")\n",
    "if lda_model is None:\n",
    "    lda_model = models.LdaModel(\n",
    "        corpus, num_topics=num_topics, id2word=dictionary, passes=15, random_state=42\n",
    "    )\n",
    "    save_checkpoint(lda_model, \"lda_model\")\n",
    "\n",
    "# Get topic distributions for each document\n",
    "topic_distributions = load_checkpoint(\"topic_distributions\")\n",
    "if topic_distributions is None:\n",
    "    topic_distributions = [\n",
    "        lda_model.get_document_topics(bow) \n",
    "        for bow in tqdm(corpus, desc=\"Getting topic distributions\")\n",
    "    ]\n",
    "    save_checkpoint(topic_distributions, \"topic_distributions\")\n",
    "\n",
    "topic_vectors = load_checkpoint(\"topic_vectors\")\n",
    "if topic_vectors is None:\n",
    "    topic_vectors = [\n",
    "        topic_vector(td, num_topics) \n",
    "        for td in tqdm(topic_distributions, desc=\"Creating topic vectors\")\n",
    "    ]\n",
    "    save_checkpoint(topic_vectors, \"topic_vectors\")\n",
    "\n",
    "# Add topic vectors to the DataFrame\n",
    "df[\"TopicVector\"] = topic_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Character Mapping and Dataset Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, texts, topic_vectors, char2idx, seq_length):\n",
    "        self.texts = texts\n",
    "        self.topic_vectors = topic_vectors\n",
    "        self.char2idx = char2idx\n",
    "        self.seq_length = seq_length\n",
    "        self.data = self.process_texts()\n",
    "\n",
    "    def process_texts(self):\n",
    "        data = []\n",
    "        for text, topic_vec in zip(self.texts, self.topic_vectors):\n",
    "            encoded = [self.char2idx.get(char, 0) for char in text]\n",
    "            if len(encoded) < self.seq_length + 1:\n",
    "                continue  # Skip sequences that are too short\n",
    "            for i in range(len(encoded) - self.seq_length):\n",
    "                input_seq = encoded[i : i + self.seq_length]\n",
    "                target_seq = encoded[i + 1 : i + self.seq_length + 1]\n",
    "                data.append((input_seq, target_seq, topic_vec))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target_seq, topic_vec = self.data[idx]\n",
    "        return (\n",
    "            torch.tensor(input_seq, dtype=torch.long),\n",
    "            torch.tensor(target_seq, dtype=torch.long),\n",
    "            torch.tensor(topic_vec, dtype=torch.float32),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create character mapping\n",
    "all_text = \" \".join(df[\"Text\"])\n",
    "chars = sorted(list(set(all_text)))\n",
    "char2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "vocab_size = len(char2idx)\n",
    "seq_length = 100  # Adjust based on your data\n",
    "\n",
    "# Prepare the dataset and dataloader\n",
    "dataset = EmailDataset(\n",
    "    df[\"Text\"].tolist(), df[\"TopicVector\"].tolist(), char2idx, seq_length\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Defining the Topic-Guided VAE Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, latent_dim, num_topics):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim + num_topics, hidden_dim, batch_first=True)\n",
    "        self.hidden_to_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.hidden_to_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x, topic_vec):\n",
    "        x = self.embedding(x)\n",
    "        topic_vec_expanded = topic_vec.unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "        x = torch.cat([x, topic_vec_expanded], dim=2)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        h_n = h_n.squeeze(0)\n",
    "        mu = self.hidden_to_mu(h_n)\n",
    "        logvar = self.hidden_to_logvar(h_n)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, latent_dim, num_topics):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_to_hidden = nn.Linear(latent_dim + num_topics, hidden_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.outputs2vocab = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x, z, topic_vec):\n",
    "        z = torch.cat([z, topic_vec], dim=1)\n",
    "        h_0 = torch.tanh(self.latent_to_hidden(z)).unsqueeze(0)\n",
    "        c_0 = torch.zeros_like(h_0).to(device)\n",
    "        x = self.embedding(x)\n",
    "        outputs, _ = self.lstm(x, (h_0, c_0))\n",
    "        logits = self.outputs2vocab(outputs)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# VAE Model\n",
    "class TopicGuidedVAE(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, latent_dim, num_topics):\n",
    "        super(TopicGuidedVAE, self).__init__()\n",
    "        self.encoder = Encoder(\n",
    "            vocab_size, embedding_dim, hidden_dim, latent_dim, num_topics\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            vocab_size, embedding_dim, hidden_dim, latent_dim, num_topics\n",
    "        )\n",
    "\n",
    "    def forward(self, x, topic_vec):\n",
    "        mu, logvar = self.encoder(x, topic_vec)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        logits = self.decoder(x, z, topic_vec)\n",
    "        return logits, mu, logvar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the Loss Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(logits, targets, mu, logvar):\n",
    "    CE = F.cross_entropy(\n",
    "        logits.view(-1, logits.size(-1)), targets.view(-1), reduction=\"mean\"\n",
    "    )\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return CE + KLD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training the VAE Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  84%|âââââââââ | 7801/9310 [08:22<01:37, 15.52it/s, loss=0.7521, ce_loss=0.7478, kld_loss=0.0043]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (input_seq, target_seq, topic_vec) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(progress_bar):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Move data to device\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     input_seq \u001b[38;5;241m=\u001b[39m \u001b[43minput_seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     target_seq \u001b[38;5;241m=\u001b[39m target_seq\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m     topic_vec \u001b[38;5;241m=\u001b[39m topic_vec\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "latent_dim = 64\n",
    "\n",
    "# Initialize model, optimizer and tensorboard writer\n",
    "model = TopicGuidedVAE(\n",
    "    vocab_size, embedding_dim, hidden_dim, latent_dim, num_topics\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "writer = SummaryWriter('runs/topic_guided_vae')\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Adjust based on your data\n",
    "best_loss = float('inf')\n",
    "model.train()\n",
    "\n",
    "# Initialize early stopping parameters\n",
    "patience = 3  # Number of epochs to wait for improvement\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total_ce_loss = 0 \n",
    "    total_kld_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch_idx, (input_seq, target_seq, topic_vec) in enumerate(progress_bar):\n",
    "        # Move data to device\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "        topic_vec = topic_vec.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits, mu, logvar = model(input_seq, topic_vec)\n",
    "        \n",
    "        # Calculate losses\n",
    "        ce_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), \n",
    "                                target_seq.view(-1), reduction=\"mean\")\n",
    "        kld_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        loss = ce_loss + kld_loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update metrics\n",
    "        total_loss += loss.item()\n",
    "        total_ce_loss += ce_loss.item()\n",
    "        total_kld_loss += kld_loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'ce_loss': f'{ce_loss.item():.4f}',\n",
    "            'kld_loss': f'{kld_loss.item():.4f}'\n",
    "        })\n",
    "\n",
    "        # Log losses every 1000 iterations\n",
    "        if batch_idx % 1000 == 0:\n",
    "            writer.add_scalar('Loss/total_step', loss.item(), epoch * len(dataloader) + batch_idx)\n",
    "            writer.add_scalar('Loss/ce_step', ce_loss.item(), epoch * len(dataloader) + batch_idx)\n",
    "            writer.add_scalar('Loss/kld_step', kld_loss.item(), epoch * len(dataloader) + batch_idx)\n",
    "\n",
    "        # Save checkpoint every 5000 batches\n",
    "        if batch_idx % 5000 == 0:\n",
    "            checkpoint_path = f'checkpoints/model_epoch{epoch}_batch{batch_idx}.pt'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'batch_idx': batch_idx,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss.item(),\n",
    "                'ce_loss': ce_loss.item(),\n",
    "                'kld_loss': kld_loss.item()\n",
    "            }, checkpoint_path)\n",
    "\n",
    "    # Calculate average losses\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_ce_loss = total_ce_loss / len(dataloader)\n",
    "    avg_kld_loss = total_kld_loss / len(dataloader)\n",
    "\n",
    "    # Log to tensorboard\n",
    "    writer.add_scalar('Loss/total', avg_loss, epoch)\n",
    "    writer.add_scalar('Loss/cross_entropy', avg_ce_loss, epoch)\n",
    "    writer.add_scalar('Loss/kld', avg_kld_loss, epoch)\n",
    "    \n",
    "    # Save best model and check for early stopping\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss,\n",
    "        }, 'checkpoints/best_model.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:  # Stop if no improvement for patience epochs\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Save epoch checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_loss,\n",
    "        'ce_loss': avg_ce_loss,\n",
    "        'kld_loss': avg_kld_loss\n",
    "    }, f'checkpoints/model_epoch{epoch}.pt')\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Cross Entropy Loss: {avg_ce_loss:.4f}\")\n",
    "    print(f\"KLD Loss: {avg_kld_loss:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generating New Emails Conditioned on Topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_email(model, idx2char, char2idx, topic_vec, start_text=\"\", length=500):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_seq = [char2idx.get(c, 0) for c in start_text]\n",
    "        input_seq = (\n",
    "            torch.tensor(input_seq[-seq_length:], dtype=torch.long)\n",
    "            .unsqueeze(0)\n",
    "            .to(device)\n",
    "        )\n",
    "        topic_vec = torch.tensor(topic_vec, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        if input_seq.size(1) < seq_length:\n",
    "            pad_size = seq_length - input_seq.size(1)\n",
    "            input_seq = F.pad(input_seq, (pad_size, 0), \"constant\", 0)\n",
    "\n",
    "        mu, logvar = model.encoder(input_seq, topic_vec)\n",
    "        z = mu  # Use mean for deterministic output\n",
    "        generated = start_text\n",
    "        for _ in range(length):\n",
    "            logits = model.decoder(input_seq, z, topic_vec)\n",
    "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "            next_char_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "            next_char = idx2char[next_char_idx]\n",
    "            generated += next_char\n",
    "            input_seq = torch.cat(\n",
    "                [input_seq[:, 1:], torch.tensor([[next_char_idx]], device=device)],\n",
    "                dim=1,\n",
    "            )\n",
    "        return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.ldamodel:topic #0 (0.200): 0.021*\"com\" + 0.020*\"https\" + 0.016*\"click\" + 0.009*\"jamanetwork\" + 0.009*\"alerts\" + 0.008*\"ks\" + 0.008*\"axac\" + 0.008*\"objz\" + 0.007*\"heygen\" + 0.006*\"aws\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.200): 0.011*\"read\" + 0.008*\"nvidia\" + 0.007*\"https\" + 0.006*\"tldr\" + 0.006*\"minute\" + 0.005*\"com\" + 0.005*\"data\" + 0.005*\"new\" + 0.004*\"utm_source\" + 0.004*\"ai\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.200): 0.004*\"obj\" + 0.004*\"endobj\" + 0.003*\"type\" + 0.003*\"null\" + 0.003*\"length\" + 0.002*\"stream\" + 0.002*\"copilot\" + 0.002*\"filter\" + 0.002*\"flatedecode\" + 0.002*\"xz\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.200): 0.015*\"github\" + 0.014*\"https\" + 0.013*\"build\" + 0.013*\"com\" + 0.012*\"data\" + 0.011*\"eigen\" + 0.009*\"root\" + 0.009*\"failed\" + 0.007*\"run\" + 0.007*\"holafly\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.200): 0.020*\"de\" + 0.014*\"https\" + 0.012*\"com\" + 0.008*\"email\" + 0.007*\"utm_source\" + 0.007*\"en\" + 0.007*\"la\" + 0.005*\"el\" + 0.005*\"read\" + 0.005*\"tldr\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics discovered by LDA:\n",
      "Topic 0: 0.021*\"com\" + 0.020*\"https\" + 0.016*\"click\" + 0.009*\"jamanetwork\" + 0.009*\"alerts\" + 0.008*\"ks\" + 0.008*\"axac\" + 0.008*\"objz\" + 0.007*\"heygen\" + 0.006*\"aws\"\n",
      "Topic 1: 0.011*\"read\" + 0.008*\"nvidia\" + 0.007*\"https\" + 0.006*\"tldr\" + 0.006*\"minute\" + 0.005*\"com\" + 0.005*\"data\" + 0.005*\"new\" + 0.004*\"utm_source\" + 0.004*\"ai\"\n",
      "Topic 2: 0.004*\"obj\" + 0.004*\"endobj\" + 0.003*\"type\" + 0.003*\"null\" + 0.003*\"length\" + 0.002*\"stream\" + 0.002*\"copilot\" + 0.002*\"filter\" + 0.002*\"flatedecode\" + 0.002*\"xz\"\n",
      "Topic 3: 0.015*\"github\" + 0.014*\"https\" + 0.013*\"build\" + 0.013*\"com\" + 0.012*\"data\" + 0.011*\"eigen\" + 0.009*\"root\" + 0.009*\"failed\" + 0.007*\"run\" + 0.007*\"holafly\"\n",
      "Topic 4: 0.020*\"de\" + 0.014*\"https\" + 0.012*\"com\" + 0.008*\"email\" + 0.007*\"utm_source\" + 0.007*\"en\" + 0.007*\"la\" + 0.005*\"el\" + 0.005*\"read\" + 0.005*\"tldr\"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Generate a new email\u001b[39;00m\n\u001b[1;32m     14\u001b[0m start_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDear \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m generated_email \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_email\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx2char\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar2idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerated Email:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_email)\n",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m, in \u001b[0;36mgenerate_email\u001b[0;34m(model, idx2char, char2idx, topic_vec, start_text, length)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     input_seq \u001b[38;5;241m=\u001b[39m [char2idx\u001b[38;5;241m.\u001b[39mget(c, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m start_text]\n\u001b[1;32m      5\u001b[0m     input_seq \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      6\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mseq_length\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     topic_vec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(topic_vec, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_seq\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m seq_length:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Display the topics\n",
    "print(\"\\nTopics discovered by LDA:\")\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "# Select a topic index (e.g., topic 0)\n",
    "selected_topic = 0\n",
    "\n",
    "# Create a one-hot topic vector\n",
    "topic_vec = np.zeros(num_topics)\n",
    "topic_vec[selected_topic] = 1.0\n",
    "\n",
    "# Generate a new email\n",
    "start_text = \"Dear \"\n",
    "generated_email = generate_email(\n",
    "    model, idx2char, char2idx, topic_vec, start_text=start_text, length=500\n",
    ")\n",
    "print(\"\\nGenerated Email:\")\n",
    "print(generated_email)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation and Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# You can further evaluate the generated emails by inspecting them manually.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Additionally, visualize topics and their associated words to understand the topics better.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Visualize topics\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_topics):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# You can further evaluate the generated emails by inspecting them manually.\n",
    "# Additionally, visualize topics and their associated words to understand the topics better.\n",
    "\n",
    "# Visualize topics\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "for idx in range(num_topics):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.title(f\"Word Cloud for Topic {idx}\")\n",
    "    words = dict(lda_model.show_topic(idx, 50))\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, height=600, background_color=\"white\"\n",
    "    ).generate_from_frequencies(words)\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
